{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from order_by.sorting import *\n",
    "from order_by.utils import *\n",
    "\n",
    "with open(\".open_ai_api_key\", \"r\") as f:\n",
    "    api_key = f.read().strip()\n",
    "\n",
    "client = OpenAI(api_key=api_key,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample and save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datasets\n",
    "\n",
    "# data = datasets.load_dataset(\"ms_marco\", \"v1.1\", split=\"train\")\n",
    "# sampled_data = data.shuffle(seed=42).select(range(100))\n",
    "\n",
    "# data_list = [dict(item) for item in sampled_data]\n",
    "# with open(\"msmarco_v1.1_sampled_100.json\", \"w\") as f:\n",
    "#     json.dump(data_list, f, indent=2)\n",
    "\n",
    "# print(\"Saved to 'msmarco_v1.1_sampled_100.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Experiment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"msmarco_v1.1_sampled_100.json\", \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointwise_prompt_template = \"\"\"You are given a question and a passage. Evaluate how well the passage answers the question by assigning a score from 0 to 5.\\nOutput only a float score.\\nQuestion: {question}\\nPassage: {passage}\\nScore:\"\"\"\n",
    "\n",
    "external_pointwise_prompt_template = \"\"\"You are given a question and a list of passages. Evaluate how well each passage answers the question. For each passage, assign a float score from 0 to 5.\\nOutput a JSON list of float scores in the same order as the input passages.\\nQuestion: {question}\\nPassages:\\n{numbered_passages}\\nScore:\"\"\"\n",
    "\n",
    "pairwise_comparison_prompt_template = \"\"\"You are given a question and two passages. Determine which passage answers the question better.\\nQuestion: {question}\\nPassage A: {passage_a}\\nPassage B: {passage_b}\\nOutput only one word: 'A' if Passage A is better, 'B' if Passage B is better, or 'Equal' if both are equally good.\"\"\"\n",
    "\n",
    "external_comparison_prompt_template = \"\"\"You are given a question and a list of passages. Rank the passages based on how well they answer the question, from best to worst.\\nQuestion: {question}\\nPassages:\\n{numbered_passages}\\nOutput a JSON list of passage numbers in ranked order (best to worst).\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_numbered_passages(passages):\n",
    "    return \"\\n\".join([f\"{i+1}. {p}\" for i, p in enumerate(passages)])\n",
    "\n",
    "def precision(selected_ground_truth, output):\n",
    "    \"\"\" Evaluate the precision of the ranked result.\n",
    "    precision@k = (#relevant output items in top k / k) where k is the number of relevant items in selected_ground_truth.\n",
    "    \"\"\"\n",
    "    selected_ground_truth = np.asarray(selected_ground_truth)\n",
    "    k = np.sum(selected_ground_truth, axis=1)\n",
    "    idxs = np.asarray(output) - 1\n",
    "    rows = np.arange(idxs.shape[0])[:, None]\n",
    "    relevance = selected_ground_truth[rows, idxs]\n",
    "    rank_positions = np.arange(idxs.shape[1])\n",
    "    mask = rank_positions < k[:, None]\n",
    "    return np.sum(mask * relevance, axis=1) / k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, s in enumerate(data):\n",
    "    query = s['query']\n",
    "    passage = s['passages']['passage_text']\n",
    "    selected = s['passages']['is_selected']\n",
    "    if sum(selected) == 0:\n",
    "        print(f\"skip bad experiment {i}\")\n",
    "        print(selected)\n",
    "        continue\n",
    "    \n",
    "    # print(pointwise_prompt_template.format(question=query, passage=passage[0]))\n",
    "    # print(external_pointwise_prompt_template.format(question=query, numbered_passages=create_numbered_passages(passage)))\n",
    "    # print(pairwise_comparison_prompt_template.format(question=query, passage_a=passage[0], passage_b=passage[1]))\n",
    "    # print(external_comparison_prompt_template.format(question=query, numbered_passages=create_numbered_passages(passage)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
